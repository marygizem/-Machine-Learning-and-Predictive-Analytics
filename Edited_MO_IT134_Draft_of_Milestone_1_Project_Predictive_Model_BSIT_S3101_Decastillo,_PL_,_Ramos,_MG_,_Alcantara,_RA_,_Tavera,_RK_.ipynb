{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "198d8fcb-b624-4636-a35e-56a223d382eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "198d8fcb-b624-4636-a35e-56a223d382eb",
        "outputId": "3294b8c9-25f3-4d21-e732-b3a12344bc50"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'customers_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1114682143.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcustomers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customers_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mproducts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"products_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtransactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transactions_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'customers_data.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "customers = pd.read_csv(\"customers_data.csv\")\n",
        "products = pd.read_csv(\"products_data.csv\")\n",
        "transactions = pd.read_csv(\"transactions_data.csv\")\n",
        "\n",
        "#View shape (rows, columns)\n",
        "print(customers.shape)\n",
        "print(products.shape)\n",
        "print(transactions.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preview Data\n",
        "\n",
        "customers.head(10)\n",
        "products.head(10)\n",
        "transactions.head(10)"
      ],
      "metadata": {
        "id": "39GklPaQ_OU9"
      },
      "id": "39GklPaQ_OU9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by merging the cleaned transactions data with customers using Company_ID. Then, we'll take that combined data and merge it with the products data using Product_ID. This will give you a comprehensive dataset that includes information from all three sources, with all transactions preserved"
      ],
      "metadata": {
        "id": "Z0SWMNryNdW_"
      },
      "id": "Z0SWMNryNdW_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge transactions_cleaned with customers on Company_ID\n",
        "merged_data = pd.merge(transactions_cleaned, customers, on='Company_ID', how='left')\n",
        "\n",
        "# Merge the result with products on Product_ID\n",
        "merged_data = pd.merge(merged_data, products, on='Product_ID', how='left')\n",
        "\n",
        "# Display the first few rows of the merged DataFrame\n",
        "display(merged_data.head())\n",
        "\n",
        "# Display the shape of the merged DataFrame\n",
        "print(f\"Shape of the merged DataFrame: {merged_data.shape}\")"
      ],
      "metadata": {
        "id": "p2N8UmamM2qs"
      },
      "id": "p2N8UmamM2qs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "products.head(10)"
      ],
      "metadata": {
        "id": "1sYdnRssGm9F"
      },
      "id": "1sYdnRssGm9F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Duplicate columns = columns with identical values.\n",
        "def duplicate_columns_info(df):\n",
        "    dup_mask = df.T.duplicated()\n",
        "    dup_columns = df.columns[dup_mask]\n",
        "\n",
        "    return {\n",
        "        \"duplicate_columns\": list(dup_columns),\n",
        "        \"duplicate_count\": dup_mask.sum()\n",
        "    }\n",
        "\n",
        "#Check duplicate columns for each file\n",
        "customers_dups = duplicate_columns_info(customers)\n",
        "transactions_dups = duplicate_columns_info(transactions)\n",
        "products_dups = duplicate_columns_info(products)\n",
        "\n",
        "customers_dups, transactions_dups, products_dups\n",
        "\n",
        "def print_duplicate_columns(name, result):\n",
        "    print(f\"\\n{name}\")\n",
        "    print(\"Duplicate column count:\", result[\"duplicate_count\"])\n",
        "    print(\"Duplicate columns:\", result[\"duplicate_columns\"])\n",
        "\n",
        "print_duplicate_columns(\"Customers Data\", customers_dups)\n",
        "print_duplicate_columns(\"Transactions Data\", transactions_dups)\n",
        "print_duplicate_columns(\"Products Data\", products_dups)\n"
      ],
      "metadata": {
        "id": "UdyVpw7wGrf0"
      },
      "id": "UdyVpw7wGrf0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop Unnamed Column as this is data is too redundant with Transaction_ID\n",
        "\n",
        "transactions.drop(columns=[\"Unnamed: 0\"], inplace=True, errors='ignore')"
      ],
      "metadata": {
        "id": "ikpIIyErElKt"
      },
      "id": "ikpIIyErElKt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customers.head(10)\n",
        "products.head(10)\n",
        "transactions.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "0BLuwU7OEuV0",
        "outputId": "a98d22de-367a-4ed2-d597-3200ebbc4f73"
      },
      "id": "0BLuwU7OEuV0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   unnamed:_0  transaction_id  company_id  product_id  quantity  \\\n",
              "0         0.0             1.0        88.0         6.0      11.0   \n",
              "1         1.0             2.0        29.0        19.0      16.0   \n",
              "2         2.0          5005.0        28.0        18.0       6.0   \n",
              "3         3.0             4.0        85.0        12.0      12.0   \n",
              "4         4.0             5.0        47.0         3.0       8.0   \n",
              "5         5.0             6.0        80.0        11.0       4.0   \n",
              "6      4997.5             7.0        50.0        10.0      14.0   \n",
              "7         7.0             8.0        21.0        20.0       7.0   \n",
              "8         8.0             9.0        96.0         1.0      14.0   \n",
              "9         9.0            10.0         3.0        20.0       6.0   \n",
              "\n",
              "  transaction_date  product_price  total_cost  high_value_transaction  \n",
              "0       2024-03-26  194379.147964   1075200.0                       0  \n",
              "1              NaN   97930.993380   1428000.0                       0  \n",
              "2              NaN  126095.547778    940800.0                       0  \n",
              "3              NaN  131297.783516   1008000.0                       0  \n",
              "4              NaN   99575.609634    705600.0                       0  \n",
              "5       2021-07-12  160658.675350    627200.0                       0  \n",
              "6       2023-11-01  133548.749710   1960000.0                       1  \n",
              "7              NaN  229217.941468   1792000.0                       0  \n",
              "8              NaN  144758.783254   1344000.0                       0  \n",
              "9              NaN  238293.851303   1120000.0                       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5f833dd-cf8d-433e-964e-92fbe3c754a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unnamed:_0</th>\n",
              "      <th>transaction_id</th>\n",
              "      <th>company_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>quantity</th>\n",
              "      <th>transaction_date</th>\n",
              "      <th>product_price</th>\n",
              "      <th>total_cost</th>\n",
              "      <th>high_value_transaction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>88.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>2024-03-26</td>\n",
              "      <td>194379.147964</td>\n",
              "      <td>1075200.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>97930.993380</td>\n",
              "      <td>1428000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.0</td>\n",
              "      <td>5005.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>126095.547778</td>\n",
              "      <td>940800.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>131297.783516</td>\n",
              "      <td>1008000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>99575.609634</td>\n",
              "      <td>705600.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2021-07-12</td>\n",
              "      <td>160658.675350</td>\n",
              "      <td>627200.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4997.5</td>\n",
              "      <td>7.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2023-11-01</td>\n",
              "      <td>133548.749710</td>\n",
              "      <td>1960000.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>229217.941468</td>\n",
              "      <td>1792000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>144758.783254</td>\n",
              "      <td>1344000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>238293.851303</td>\n",
              "      <td>1120000.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5f833dd-cf8d-433e-964e-92fbe3c754a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d5f833dd-cf8d-433e-964e-92fbe3c754a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d5f833dd-cf8d-433e-964e-92fbe3c754a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "transactions",
              "summary": "{\n  \"name\": \"transactions\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"unnamed:_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2737.250768048707,\n        \"min\": 0.0,\n        \"max\": 9999.0,\n        \"num_unique_values\": 9001,\n        \"samples\": [\n          8828.0,\n          1282.0,\n          649.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transaction_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2746.396291528762,\n        \"min\": 1.0,\n        \"max\": 9999.0,\n        \"num_unique_values\": 9001,\n        \"samples\": [\n          8830.0,\n          1279.0,\n          647.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"company_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27.418179017105786,\n        \"min\": 1.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          45.0,\n          17.0,\n          38.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.473939348513085,\n        \"min\": 1.0,\n        \"max\": 20.0,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          6.0,\n          15.0,\n          8.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quantity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.511794370050226,\n        \"min\": 0.0,\n        \"max\": 21.0,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          11.0,\n          3.0,\n          17.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transaction_date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1196,\n        \"samples\": [\n          \"2021-07-25\",\n          \"2022-12-29\",\n          \"2024-10-15\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"product_price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37067.23683806008,\n        \"min\": 75613.36292254257,\n        \"max\": 246279.0503348639,\n        \"num_unique_values\": 9001,\n        \"samples\": [\n          176528.72402256756,\n          140672.5596836753,\n          203397.4441090916\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_cost\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 862330.9763194586,\n        \"min\": 84000.0,\n        \"max\": 4480000.0,\n        \"num_unique_values\": 206,\n        \"samples\": [\n          1814400.0,\n          1120000.0,\n          140000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"high_value_transaction\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardize the Transaction Date format to better understand column\n",
        "\n",
        "transactions[\"Transaction_Date\"] = pd.to_datetime(transactions[\"Transaction_Date\"], errors='coerce')\n",
        "\n",
        "transactions.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "NFFsPCUjE4B7",
        "outputId": "36632b4d-d3a4-42dd-d11d-3c01bc929fcc"
      },
      "id": "NFFsPCUjE4B7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Transaction_Date'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Transaction_Date'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-771123302.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Standardize the Transaction Date format to better understand column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtransactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Transaction_Date\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Transaction_Date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtransactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Transaction_Date'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1abf2aa1"
      },
      "source": [
        "### Imputing Missing `Transaction_Date` values with the Mode\n",
        "\n",
        "Given the high percentage of `NaT` values in `Transaction_Date` (4329 out of 9000 rows in `transactions_cleaned`), dropping these rows would result in substantial data loss. A common strategy to handle such a situation is to impute the missing values with the **mode** (most frequent value) of the column. This approach helps preserve the dataset size while assigning a statistically representative date.\n",
        "\n",
        "First, let's find the most frequent `Transaction_Date`."
      ],
      "id": "1abf2aa1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1079c81a"
      },
      "source": [
        "# Calculate the mode of the 'Transaction_Date' column in transactions_cleaned\n",
        "# .mode()[0] is used because mode() can return multiple modes if they have the same frequency\n",
        "most_frequent_date = transactions_cleaned['Transaction_Date'].mode()[0]\n",
        "\n",
        "print(f\"The most frequent transaction date is: {most_frequent_date}\")"
      ],
      "id": "1079c81a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72f020f8"
      },
      "source": [
        "Now, we will fill the `NaT` values in the `Transaction_Date` column with this most frequent date."
      ],
      "id": "72f020f8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "645f5494"
      },
      "source": [
        "# Fill NaT values in 'Transaction_Date' with the most frequent date\n",
        "transactions_cleaned['Transaction_Date'].fillna(most_frequent_date, inplace=True)\n",
        "\n",
        "# Verify that there are no more NaT values in 'Transaction_Date'\n",
        "print(\"Missing values in 'Transaction_Date' after imputation:\")\n",
        "display(transactions_cleaned['Transaction_Date'].isna().sum())\n",
        "\n",
        "# Display the first few rows of the updated DataFrame to see the changes\n",
        "display(transactions_cleaned.head())"
      ],
      "id": "645f5494",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c282379"
      },
      "source": [
        "# Display summary statistics for numerical columns in merged_data\n",
        "display(merged_data.describe())"
      ],
      "id": "8c282379",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Count missing values per column\n",
        "\n",
        "transactions.isna().sum()"
      ],
      "metadata": {
        "id": "1Bkwi7EvBg3P"
      },
      "id": "1Bkwi7EvBg3P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the merged DataFrame to a CSV file\n",
        "merged_data.to_csv('merged_data.csv', index=False)\n",
        "\n",
        "print(\"merged_data.csv has been successfully created and saved.\")"
      ],
      "metadata": {
        "id": "cMdaRrEzREzg"
      },
      "id": "cMdaRrEzREzg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eecc2606"
      },
      "source": [
        "print(\"Missing values in 'Transaction_Date' column of merged_data:\")\n",
        "display(merged_data['Transaction_Date'].isna().sum())"
      ],
      "id": "eecc2606",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba2d231d"
      },
      "source": [
        "# Task\n",
        "Analyze the `merged_data` DataFrame by checking data types, visualizing missing values and distributions of numerical columns, inspecting categorical columns for inconsistencies, and identifying redundant or irrelevant columns, especially `Product_Price_x` and `Product_Price_y`."
      ],
      "id": "ba2d231d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c473737b"
      },
      "source": [
        "## Check Data Types\n",
        "\n",
        "### Subtask:\n",
        "Examine the data types of all columns in `merged_data` using `.info()` to identify any columns with incorrect or mixed data types.\n"
      ],
      "id": "c473737b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "410abea9"
      },
      "source": [
        "**Reasoning**:\n",
        "To examine the data types of all columns in `merged_data` and identify any columns with incorrect or mixed data types, I will use the `.info()` method on the `merged_data` DataFrame.\n",
        "\n"
      ],
      "id": "410abea9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f5e9ecf"
      },
      "source": [
        "merged_data.info()"
      ],
      "id": "0f5e9ecf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec1fd9c3"
      },
      "source": [
        "## Analyze Missing Values\n",
        "\n",
        "### Subtask:\n",
        "Identify and quantify missing values across all columns in `merged_data` using `.isna().sum()` and visualize them to understand their distribution.\n"
      ],
      "id": "ec1fd9c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab0b3ab2"
      },
      "source": [
        "**Reasoning**:\n",
        "To visualize the distribution of missing values, I will create a heatmap using seaborn.heatmap() on the boolean DataFrame indicating missing values in `merged_data`.\n",
        "\n"
      ],
      "id": "ab0b3ab2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de6c9b8b"
      },
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(merged_data.isna(), cbar=False, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap in merged_data')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Rows')\n",
        "plt.show()"
      ],
      "id": "de6c9b8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "897083fb"
      },
      "source": [
        "**Reasoning**:\n",
        "To quantify the missing values, I will use `.isna().sum()` on the `merged_data` DataFrame, which will display the count of missing values for each column.\n",
        "\n"
      ],
      "id": "897083fb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18c88c99"
      },
      "source": [
        "print(\"Missing values per column in merged_data:\")\n",
        "display(merged_data.isna().sum())"
      ],
      "id": "18c88c99",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b5a0959"
      },
      "source": [
        "## Inspect Numerical Columns for Outliers and Distribution\n",
        "\n",
        "### Subtask:\n",
        "Generate descriptive statistics and visualize the distributions of numerical columns in `merged_data` to detect outliers and understand their spread.\n"
      ],
      "id": "6b5a0959"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "397bcc4f"
      },
      "source": [
        "**Reasoning**:\n",
        "To display descriptive statistics for all numerical columns in `merged_data`, I will use the `.describe()` method as specified in the instructions. This will provide a summary of central tendency, dispersion, and shape of the distribution of each numerical column.\n",
        "\n"
      ],
      "id": "397bcc4f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82fd3196"
      },
      "source": [
        "print(\"Descriptive statistics for numerical columns in merged_data:\")\n",
        "display(merged_data.describe())"
      ],
      "id": "82fd3196",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3baeb1b1"
      },
      "source": [
        "**Reasoning**:\n",
        "To visualize the distributions of numerical columns and detect outliers, I will create histograms and box plots for the specified numerical columns: Transaction_ID, Company_ID, Product_ID, Quantity, Product_Price_x, Total_Cost, and Company_Profit. Histograms will show the shape of the distributions, while box plots will highlight the spread and potential outliers.\n",
        "\n"
      ],
      "id": "3baeb1b1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fffeaee7"
      },
      "source": [
        "numerical_cols = ['Transaction_ID', 'Company_ID', 'Product_ID', 'Quantity', 'Product_Price_x', 'Total_Cost', 'Company_Profit']\n",
        "\n",
        "# Create histograms for numerical columns\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(3, 3, i + 1) # Arrange plots in a 3x3 grid\n",
        "    sns.histplot(merged_data[col].dropna(), kde=True)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create box plots for numerical columns\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    plt.subplot(3, 3, i + 1) # Arrange plots in a 3x3 grid\n",
        "    sns.boxplot(y=merged_data[col].dropna())\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "    plt.ylabel(col)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fffeaee7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d1b429b"
      },
      "source": [
        "## Inspect Categorical Columns for Inconsistencies\n",
        "\n",
        "### Subtask:\n",
        "Identify categorical columns in `merged_data` and examine their unique values and value counts to detect inconsistencies, typos, or unexpected entries."
      ],
      "id": "0d1b429b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9295b87f"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify categorical columns and prepare for inspection, I will first get a list of columns with 'object' data type from the `merged_data` DataFrame.\n",
        "\n"
      ],
      "id": "9295b87f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ef68874"
      },
      "source": [
        "categorical_cols = merged_data.select_dtypes(include='object').columns\n",
        "print(\"Categorical columns identified:\")\n",
        "print(categorical_cols)"
      ],
      "id": "0ef68874",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df117a72"
      },
      "source": [
        "**Reasoning**:\n",
        "To detect inconsistencies, typos, or unexpected entries, I will iterate through each identified categorical column and display its unique values and their counts.\n",
        "\n"
      ],
      "id": "df117a72"
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "689eecd3"
      },
      "source": [
        "for col in categorical_cols:\n",
        "    print(f\"\\nUnique values and their counts for column '{col}':\")\n",
        "    display(merged_data[col].value_counts(dropna=False))\n"
      ],
      "id": "689eecd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "899bd39f"
      },
      "source": [
        "## Identify Redundant or Irrelevant Columns\n",
        "\n",
        "### Subtask:\n",
        "Identify and address redundant or irrelevant columns, specifically focusing on `Product_Price_x` and `Product_Price_y` in `merged_data`."
      ],
      "id": "899bd39f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6db9fc15"
      },
      "source": [
        "## Identify Redundant or Irrelevant Columns\n",
        "\n",
        "### Subtask:\n",
        "Identify and address redundant or irrelevant columns, specifically focusing on `Product_Price_x` and `Product_Price_y` in `merged_data`."
      ],
      "id": "6db9fc15"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff177c7f"
      },
      "source": [
        "**Reasoning**:\n",
        "To better understand the relationship and potential redundancy between `Product_Price_x` and `Product_Price_y`, I will display the first few rows of the `merged_data` DataFrame focusing on these two columns along with `Product_ID` and `Product_Name`.\n",
        "\n"
      ],
      "id": "ff177c7f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ead69018"
      },
      "source": [
        "print(\"Comparing 'Product_Price_x' and 'Product_Price_y':\")\n",
        "display(merged_data[['Product_ID', 'Product_Name', 'Product_Price_x', 'Product_Price_y']].head(10))"
      ],
      "id": "ead69018",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c2a8c86"
      },
      "source": [
        "**Reasoning**:\n",
        "To make `Product_Price_y` comparable with `Product_Price_x` and usable for numerical analysis, I need to clean it by removing non-numeric characters (like '?' and ',') and then convert its data type to a float. This will standardize the price format across both columns.\n",
        "\n"
      ],
      "id": "4c2a8c86"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d8e9e4a"
      },
      "source": [
        "#Cleaned Product_Price_y by cleaning and removing on-numeric characters (like '?' and ',') and then convert its data type to a float. This will standardize the price format across both columns.\n",
        "\n",
        "merged_data['Product_Price_y_cleaned'] = merged_data['Product_Price_y'].astype(str).str.replace('?', '', regex=False).str.replace(',', '', regex=False)\n",
        "merged_data['Product_Price_y_cleaned'] = pd.to_numeric(merged_data['Product_Price_y_cleaned'], errors='coerce')\n",
        "\n",
        "print(\"Data types after cleaning Product_Price_y:\")\n",
        "display(merged_data[['Product_Price_x', 'Product_Price_y', 'Product_Price_y_cleaned']].dtypes)\n",
        "print(\"First 10 rows after cleaning Product_Price_y:\")\n",
        "display(merged_data[['Product_ID', 'Product_Name', 'Product_Price_x', 'Product_Price_y', 'Product_Price_y_cleaned']].head(10))"
      ],
      "id": "4d8e9e4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4058d1e2"
      },
      "source": [
        "**Reasoning**:\n",
        "To quantify the relationship and potential redundancy between `Product_Price_x` and the newly cleaned `Product_Price_y_cleaned`, I will calculate their correlation coefficient. A high correlation would suggest they represent the same information, making one of them redundant.\n",
        "\n"
      ],
      "id": "4058d1e2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f611e3c"
      },
      "source": [
        "#Check correlation between Product_Price_x and Product_Price_y_cleaned\n",
        "\n",
        "correlation = merged_data['Product_Price_x'].corr(merged_data['Product_Price_y_cleaned'])\n",
        "print(f\"Correlation between Product_Price_x and Product_Price_y_cleaned: {correlation}\")\n",
        "\n",
        "# Also, visually compare them with a scatter plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='Product_Price_x', y='Product_Price_y_cleaned', data=merged_data)\n",
        "plt.title('Scatter Plot of Product_Price_x vs. Product_Price_y_cleaned')\n",
        "plt.xlabel('Product_Price_x')\n",
        "plt.ylabel('Product_Price_y_cleaned')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "3f611e3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0be473c1"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the correlation between 'Product_Price_x' and 'Product_Price_y_cleaned' is very high (0.979), indicating strong redundancy, I will drop the 'Product_Price_y' and 'Product_Price_y_cleaned' columns, keeping 'Product_Price_x' as the primary price column.\n",
        "\n"
      ],
      "id": "0be473c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe3983f5"
      },
      "source": [
        "\n",
        "\n",
        "merged_data.drop(columns=['Product_Price_y', 'Product_Price_y_cleaned'], inplace=True, errors='ignore')\n",
        "print(\"Dropped 'Product_Price_y' and 'Product_Price_y_cleaned' columns (if they existed).\")\n",
        "print(\"Remaining columns:\")\n",
        "display(merged_data.columns)"
      ],
      "id": "fe3983f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1172c74"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Types and Potential Issues**:\n",
        "    *   `Transaction_Date` is currently an `object` type and requires conversion to a datetime format for time-series analysis.\n",
        "    *   `Product_Price_y` was initially an `object` type, indicating non-numeric characters or string storage, necessitating cleaning and conversion to a numeric type. `Product_Price_x` is already a `float64`.\n",
        "*   **Missing Values**:\n",
        "    *   `Company_Profit` has the highest number of missing values with 1148.\n",
        "    *   `Company_Name` and `Address` both have 536 missing values.\n",
        "    *   `Product_Name` and `Product_Price_y` both have 530 missing values.\n",
        "    *   `Transaction_ID`, `Company_ID`, `Product_ID`, `Quantity`, `Transaction_Date`, `Product_Price_x`, and `Total_Cost` have no missing values.\n",
        "*   **Numerical Column Distributions**:\n",
        "    *   `Company_Profit` has a lower count of 4181 non-null entries out of 5329 total, confirming the presence of missing values.\n",
        "    *   The `Quantity` column includes a minimum value of 0, which may represent specific business cases (e.g., returns or failed transactions) or data entry errors.\n",
        "    *   `Product_Price_x` and `Total_Cost` show large standard deviations, indicating a wide spread in their values.\n",
        "    *   Box plots revealed potential outliers in `Transaction_ID`, `Product_Price_x`, `Total_Cost`, and `Company_Profit`.\n",
        "*   **Redundant Columns (`Product_Price_x` vs. `Product_Price_y`)**:\n",
        "    *   `Product_Price_y` contained non-numeric characters such as '?' and ',', making it inconsistent with `Product_Price_x`.\n",
        "    *   After cleaning `Product_Price_y` into a new numeric column `Product_Price_y_cleaned`, its correlation with `Product_Price_x` was assessed.\n",
        "    *   Ultimately, the original `Product_Price_y` and the cleaned `Product_Price_y_cleaned` columns were dropped, implying `Product_Price_x` was selected as the primary and more reliable product price column.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Convert the `Transaction_Date` column to a datetime format to enable proper time-series analysis and calculations.\n",
        "*   Develop a strategy to handle the significant missing values, particularly in `Company_Profit`, `Company_Name`, `Address`, and `Product_Name`, considering imputation, deletion, or further investigation into the cause of missingness.\n",
        "*   Investigate the minimum `Quantity` of 0 to understand its business implications and decide whether these entries should be filtered, adjusted, or kept based on context.\n"
      ],
      "id": "a1172c74"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2d71c6e"
      },
      "source": [
        "# Task\n",
        "Convert 'Transaction_Date' to a datetime format in the `merged_data` DataFrame, then address missing values in 'Company_Profit', 'Company_Name', 'Address', and 'Product_Name' in `merged_data`. Finally, investigate entries where 'Quantity' is 0 in `merged_data`."
      ],
      "id": "f2d71c6e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ccbc6c4"
      },
      "source": [
        "## Date Type Conversion\n",
        "\n",
        "### Subtask:\n",
        "Convert 'Transaction_Date' to a datetime format to enable time-series analysis and feature extraction.\n"
      ],
      "id": "5ccbc6c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07c33d37"
      },
      "source": [
        "**Reasoning**:\n",
        "To convert the 'Transaction_Date' column to a datetime format and handle potential errors, I will use `pd.to_datetime` with `errors='coerce'`. After the conversion, I will verify the data type using `merged_data.info()` to ensure the change was successful.\n",
        "\n"
      ],
      "id": "07c33d37"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5aa00c8"
      },
      "source": [
        "merged_data['Transaction_Date'] = pd.to_datetime(merged_data['Transaction_Date'], errors='coerce')\n",
        "\n",
        "print(\"Data type after converting 'Transaction_Date' to datetime:\")\n",
        "merged_data.info()"
      ],
      "id": "b5aa00c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3afe8c8a"
      },
      "source": [
        "## Handle Missing Values\n",
        "\n",
        "### Subtask:\n",
        "Address missing values in 'Company_Profit', 'Company_Name', 'Address', and 'Product_Name' in `merged_data`.\n",
        "\n",
        "#### Reasoning and Strategy:\n",
        "\n",
        "*   **Company_Profit (Numerical):** Given its numerical nature, a suitable imputation strategy would be to fill missing values with the **median**. The median is less sensitive to outliers compared to the mean, making it a robust choice for financial data like profit.\n",
        "\n",
        "*   **Company_Name (Categorical/Object):** Missing company names could indicate unknown entities or data entry issues. Imputing with a placeholder like **'Unknown'** is a common and safe approach for categorical data, as it preserves the information that the original name was missing without introducing false relationships.\n",
        "\n",
        "*   **Address (Categorical/Object):** Similar to `Company_Name`, missing addresses can be imputed with **'Unknown'** to explicitly mark the absence of this information, preventing potential geographical or logistical biases in analysis.\n",
        "\n",
        "*   **Product_Name (Categorical/Object):** For missing product names, imputing with **'Unknown Product'** allows us to retain these transaction records while acknowledging that the specific product information is unavailable. This prevents the loss of potentially valuable transaction data.\n",
        "\n",
        "**Note:** Before imputation, I will re-check the missing values count to ensure I am targeting the correct columns and have the latest information."
      ],
      "id": "3afe8c8a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a5ac3b1"
      },
      "source": [
        "**Reasoning**:\n",
        "Before imputing missing values, I will re-check the current count of missing values for each column in `merged_data` to ensure the strategy is based on the most up-to-date information.\n",
        "\n"
      ],
      "id": "9a5ac3b1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cb2d4c6"
      },
      "source": [
        "#Check Missing Values per column\n",
        "\n",
        "print(\"Missing values per column in merged_data before imputation:\")\n",
        "display(merged_data.isna().sum())"
      ],
      "id": "2cb2d4c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec97719a"
      },
      "source": [
        "**Reasoning**:\n",
        "To address the `FutureWarning` related to `inplace=True` when using `fillna()`, I will rewrite the code to explicitly reassign the filled columns, which is a more robust and recommended practice in pandas.\n",
        "\n"
      ],
      "id": "ec97719a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61edd635"
      },
      "source": [
        "median_company_profit = merged_data['Company_Profit'].median()\n",
        "merged_data['Company_Profit'] = merged_data['Company_Profit'].fillna(median_company_profit)\n",
        "\n",
        "merged_data['Company_Name'] = merged_data['Company_Name'].fillna('Unknown')\n",
        "merged_data['Address'] = merged_data['Address'].fillna('Unknown')\n",
        "merged_data['Product_Name'] = merged_data['Product_Name'].fillna('Unknown Product')\n",
        "\n",
        "print(\"Missing values per column in merged_data after imputation:\")\n",
        "display(merged_data.isna().sum())"
      ],
      "id": "61edd635",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5641b500"
      },
      "source": [
        "## Investigate Quantity = 0\n",
        "\n",
        "### Subtask:\n",
        "Investigate entries in `merged_data` where the 'Quantity' column has a value of 0 to understand their implications and decide on appropriate handling.\n",
        "\n",
        "#### Reasoning and Strategy:\n",
        "\n",
        "A minimum `Quantity` of 0 could represent various scenarios such as cancelled orders, returns, or data entry errors. It's crucial to understand the proportion and context of these entries to determine the best course of action. Strategies could include:\n",
        "\n",
        "*   **Filtering out**: If these entries genuinely represent non-sales events, removing them might be appropriate for sales analysis.\n",
        "*   **Further investigation**: If they represent returns or specific business cases, they might need to be treated differently (e.g., recorded as negative quantities or analyzed separately).\n",
        "\n",
        "First, I will quantify how many records have a `Quantity` of 0 and examine their characteristics."
      ],
      "id": "5641b500"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8acfd452"
      },
      "source": [
        "**Reasoning**:\n",
        "To quantify the records with a 'Quantity' of 0, I will filter the `merged_data` DataFrame for these entries, calculate their count, and display the first few rows to understand their characteristics.\n",
        "\n"
      ],
      "id": "8acfd452"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1acddc11"
      },
      "source": [
        "quantity_zero_entries = merged_data[merged_data['Quantity'] == 0]\n",
        "\n",
        "print(f\"Number of records where Quantity is 0: {len(quantity_zero_entries)}\")\n",
        "print(\"First 10 records where Quantity is 0:\")\n",
        "display(quantity_zero_entries.head(10))\n"
      ],
      "id": "1acddc11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "106ae41d"
      },
      "source": [
        "## Feature Engineering: Time-Based Features\n",
        "\n",
        "### Subtask:\n",
        "Extract features such as year, month, day of week, and possibly quarter from the 'Transaction_Date' to capture temporal patterns.\n"
      ],
      "id": "106ae41d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13693ca7"
      },
      "source": [
        "**Reasoning**:\n",
        "To extract time-based features from the 'Transaction_Date' column, I will create new columns for the year, month, day of the week, and quarter using pandas' datetime properties, and then display the head of the DataFrame to verify the new columns.\n",
        "\n"
      ],
      "id": "13693ca7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fefbbf33"
      },
      "source": [
        "merged_data['Transaction_Year'] = merged_data['Transaction_Date'].dt.year\n",
        "merged_data['Transaction_Month'] = merged_data['Transaction_Date'].dt.month\n",
        "merged_data['Transaction_DayOfWeek'] = merged_data['Transaction_Date'].dt.dayofweek\n",
        "merged_data['Transaction_Quarter'] = merged_data['Transaction_Date'].dt.quarter\n",
        "\n",
        "print(\"First 10 rows of merged_data with new time-based features:\")\n",
        "display(merged_data.head(10))"
      ],
      "id": "fefbbf33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d34a39c"
      },
      "source": [
        "## Feature Engineering: Customer-Centric Aggregates\n",
        "\n",
        "### Subtask:\n",
        "Create features like total number of transactions per customer, total quantity purchased, total spending, and average transaction value per customer.\n"
      ],
      "id": "8d34a39c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3934bc5e"
      },
      "source": [
        "**Reasoning**:\n",
        "To create customer-centric aggregate features, I will group the `merged_data` DataFrame by `Company_ID` and then calculate the total number of transactions, total quantity purchased, total spending, and average transaction value for each company using the `agg()` method.\n",
        "\n"
      ],
      "id": "3934bc5e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f831c86"
      },
      "source": [
        "customer_aggregates = merged_data.groupby('Company_ID').agg(\n",
        "    total_transactions=('Transaction_ID', 'count'),\n",
        "    total_quantity_purchased=('Quantity', 'sum'),\n",
        "    total_spending=('Total_Cost', 'sum'),\n",
        "    average_transaction_value=('Total_Cost', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "print(\"First few rows of the customer_aggregates DataFrame:\")\n",
        "display(customer_aggregates.head())"
      ],
      "id": "8f831c86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c47d2da4"
      },
      "source": [
        "## Feature Engineering: Product-Centric Aggregates\n",
        "\n",
        "### Subtask:\n",
        "Develop features such as total quantity sold per product, total revenue generated by each product, and average product price.\n"
      ],
      "id": "c47d2da4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3688ca7f"
      },
      "source": [
        "**Reasoning**:\n",
        "To develop product-centric features, I will group the `merged_data` DataFrame by 'Product_ID' and calculate the total quantity sold, total revenue, and average product price using the `agg()` method.\n",
        "\n"
      ],
      "id": "3688ca7f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "485ebce8"
      },
      "source": [
        "product_aggregates = merged_data.groupby('Product_ID').agg(\n",
        "    total_quantity_sold=('Quantity', 'sum'),\n",
        "    total_revenue=('Total_Cost', 'sum'),\n",
        "    average_product_price=('Product_Price_x', 'mean')\n",
        ").reset_index()\n",
        "\n",
        "print(\"First few rows of the product_aggregates DataFrame:\")\n",
        "display(product_aggregates.head())"
      ],
      "id": "485ebce8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ebec703"
      },
      "source": [
        "## Outlier Treatment (Conditional)\n",
        "\n",
        "### Subtask:\n",
        "Based on the observed distributions and potential outliers in numerical columns ('Transaction_ID', 'Product_Price_x', 'Total_Cost', 'Company_Profit'), apply appropriate outlier handling techniques if necessary for model robustness.\n"
      ],
      "id": "4ebec703"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019a2182"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **Was 'Transaction_Date' successfully converted to a datetime format?**\n",
        "    Yes, the 'Transaction_Date' column was successfully converted to `datetime64[ns]` format, enabling time-series analysis.\n",
        "*   **Were missing values in 'Company_Profit', 'Company_Name', 'Address', and 'Product_Name' addressed?**\n",
        "    Yes, missing values in these columns were successfully addressed using appropriate imputation strategies: median for 'Company\\_Profit', and 'Unknown' or 'Unknown Product' for the categorical columns ('Company\\_Name', 'Address', 'Product\\_Name'). All these columns now have zero missing values.\n",
        "*   **Were entries where 'Quantity' is 0 investigated?**\n",
        "    Yes, it was identified that 88 records have a 'Quantity' of 0. These records still contain non-zero values in other financial columns, suggesting they might represent specific transaction types like cancelled orders or returns.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The 'Transaction\\_Date' column was successfully converted to a `datetime64[ns]` data type.\n",
        "*   Before imputation, 'Company\\_Profit' had 1148 missing values, 'Company\\_Name' and 'Address' each had 536 missing values, and 'Product\\_Name' had 530 missing values.\n",
        "*   Missing values in 'Company\\_Profit' were imputed with the median. Missing values in 'Company\\_Name' and 'Address' were imputed with 'Unknown', while 'Product\\_Name' was imputed with 'Unknown Product'. After imputation, all targeted columns had 0 missing values.\n",
        "*   A total of 88 records were found where 'Quantity' was 0. These records retained financial values in 'Product\\_Price\\_x', 'Total\\_Cost', and 'Company\\_Profit', indicating they are not simply empty entries.\n",
        "*   Four new time-based features were successfully extracted from 'Transaction\\_Date': 'Transaction\\_Year', 'Transaction\\_Month', 'Transaction\\_DayOfWeek', and 'Transaction\\_Quarter'.\n",
        "*   Customer-centric aggregate features were created for each unique `Company_ID`: `total_transactions`, `total_quantity_purchased`, `total_spending`, and `average_transaction_value`. For instance, `Company_ID` 1.0 shows 33 transactions, a total quantity of 347.0, total spending of \\$45,029,600.0, and an average transaction value of approximately \\$1.36 million.\n",
        "*   Product-centric aggregate features were developed for each unique `Product_ID`: `total_quantity_sold`, `total_revenue`, and `average_product_price`. For example, `Product_ID` 1.0 has a `total_quantity_sold` of 2443.0, a `total_revenue` of \\$344,680,000.0, and an `average_product_price` of \\$139,805.89.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The 'Quantity' = 0 records warrant further investigation to determine if they should be filtered out for sales analysis or if they represent important business events like returns or order cancellations, requiring specific handling (e.g., negative quantities for returns).\n",
        "*   The newly engineered time-based, customer-centric, and product-centric features provide a rich foundation for advanced analysis, such as trend forecasting, customer segmentation, and product performance evaluation. These features should be merged back into the main DataFrame for subsequent modeling.\n"
      ],
      "id": "019a2182"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draft of Milestone 1: Project Predictive Model\n",
        "\n"
      ],
      "metadata": {
        "id": "xYr7jgNAc9by"
      },
      "id": "xYr7jgNAc9by"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ef1c6a9"
      },
      "source": [
        "## Prepare Features for Modeling\n",
        "\n",
        "Merge the customer-centric and product-centric aggregate features into the `merged_data` DataFrame, and then prepare all features for modeling by identifying and segregating numerical and categorical features for subsequent processing.\n",
        "\n",
        "### Subtask:\n",
        "Selecting the relevant features from the `merged_data` DataFrame and the newly created aggregate DataFrames (`customer_aggregates`, `product_aggregates`).\n",
        "\n",
        "\n"
      ],
      "id": "9ef1c6a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73fd09f2"
      },
      "source": [
        "**Reasoning**:\n",
        "To select relevant features, I will first display the column names of the `merged_data` DataFrame to understand its current structure, which includes transaction-level and time-based features.\n",
        "\n"
      ],
      "id": "73fd09f2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97ceec00"
      },
      "source": [
        "print(\"Columns in merged_data DataFrame:\")\n",
        "display(merged_data.columns)"
      ],
      "id": "97ceec00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27b0b4f7"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify the customer-centric features available for merging, I will display the column names of the `customer_aggregates` DataFrame.\n",
        "\n"
      ],
      "id": "27b0b4f7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0316f1b"
      },
      "source": [
        "print(\"Columns in customer_aggregates DataFrame:\")\n",
        "display(customer_aggregates.columns)"
      ],
      "id": "c0316f1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9a1571f"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify the product-centric features available for merging, I will display the column names of the `product_aggregates` DataFrame.\n",
        "\n"
      ],
      "id": "b9a1571f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "873c5c5d"
      },
      "source": [
        "print(\"Columns in product_aggregates DataFrame:\")\n",
        "display(product_aggregates.columns)"
      ],
      "id": "873c5c5d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6f60e95"
      },
      "source": [
        "#### Merging Keys Identified\n",
        "\n",
        "*   **Customer-centric features** from `customer_aggregates` will be merged with `merged_data` using the **`Company_ID`** column.\n",
        "*   **Product-centric features** from `product_aggregates` will be merged with `merged_data` using the **`Product_ID`** column."
      ],
      "id": "f6f60e95"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ddb32f0"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the merging keys are identified, I will proceed to merge the customer-centric and product-centric aggregate features into the `merged_data` DataFrame. Merging customer aggregates first, then product aggregates.\n",
        "\n"
      ],
      "id": "4ddb32f0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f2f20f8"
      },
      "source": [
        "merged_data = pd.merge(merged_data, customer_aggregates, on='Company_ID', how='left')\n",
        "merged_data = pd.merge(merged_data, product_aggregates, on='Product_ID', how='left')\n",
        "\n",
        "print(\"First 5 rows of merged_data after adding aggregate features:\")\n",
        "display(merged_data.head())\n",
        "print(\"Shape of merged_data after adding aggregate features:\")\n",
        "display(merged_data.shape)"
      ],
      "id": "6f2f20f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48e8c8a2"
      },
      "source": [
        "Now that the merged_data DataFrame is complete with all relevant features, I will identify and separate the numerical and categorical columns. This segregation is crucial for preprocessing steps such as scaling numerical features and encoding categorical features, which are necessary before model training.\n",
        "\n"
      ],
      "id": "48e8c8a2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d31a40f7"
      },
      "source": [
        "numerical_features = merged_data.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = merged_data.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "print(\"Numerical Features:\")\n",
        "display(numerical_features)\n",
        "print(\"\\nCategorical Features:\")\n",
        "display(categorical_features)"
      ],
      "id": "d31a40f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb1c0b1a"
      },
      "source": [
        "## Encode Categorical Features\n",
        "\n",
        "### Subtask:\n",
        "Apply appropriate encoding techniques (e.g., One-Hot Encoding or Label Encoding) to categorical features such as 'Company_Name', 'Address', and 'Product_Name', and temporal features like 'Transaction_Month', 'Transaction_DayOfWeek', and 'Transaction_Quarter', if treated as categorical.\n"
      ],
      "id": "cb1c0b1a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d10f2a0"
      },
      "source": [
        "**Reasoning**:\n",
        "To encode the specified categorical features ('Company_Name', 'Address', 'Product_Name', 'Transaction_Month', 'Transaction_DayOfWeek', 'Transaction_Quarter') using one-hot encoding, I will first ensure the temporal features are treated as categorical, then apply `pd.get_dummies()`, concatenate the result with the original DataFrame, and finally drop the original columns.\n",
        "\n"
      ],
      "id": "1d10f2a0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93dc3d9c"
      },
      "source": [
        "categorical_cols_to_encode = ['Company_Name', 'Address', 'Product_Name', 'Transaction_Month', 'Transaction_DayOfWeek', 'Transaction_Quarter']\n",
        "\n",
        "# Convert temporal features to object type to treat them as categorical for one-hot encoding\n",
        "for col in ['Transaction_Month', 'Transaction_DayOfWeek', 'Transaction_Quarter']:\n",
        "    merged_data[col] = merged_data[col].astype('object')\n",
        "\n",
        "# Apply one-hot encoding\n",
        "encoded_features = pd.get_dummies(merged_data[categorical_cols_to_encode], drop_first=False)\n",
        "\n",
        "# Concatenate the encoded features with the original DataFrame\n",
        "merged_data = pd.concat([merged_data, encoded_features], axis=1)\n",
        "\n",
        "# Drop the original categorical columns\n",
        "merged_data.drop(columns=categorical_cols_to_encode, inplace=True)\n",
        "\n",
        "print(\"First 5 rows of merged_data after one-hot encoding:\")\n",
        "display(merged_data.head())\n",
        "print(\"Shape of merged_data after one-hot encoding:\")\n",
        "display(merged_data.shape)"
      ],
      "id": "93dc3d9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ed487c4"
      },
      "source": [
        "## Handle Numerical Features\n",
        "\n",
        "### Subtask:\n",
        "Ensure numerical features are suitable for modeling, potentially involving scaling or standardization.\n"
      ],
      "id": "2ed487c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7db610b"
      },
      "source": [
        "**Reasoning**:\n",
        "To ensure numerical features are suitable for modeling, I will first identify all numerical columns in the `merged_data` DataFrame after one-hot encoding, as the one-hot encoding process adds many new numerical columns.\n",
        "\n"
      ],
      "id": "c7db610b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15f44a38"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "current_numerical_features = merged_data.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "print(\"Numerical Features after one-hot encoding:\")\n",
        "display(current_numerical_features)"
      ],
      "id": "15f44a38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0773ee56"
      },
      "source": [
        "**Reasoning**:\n",
        "To standardize the numerical features, I will import `StandardScaler`, instantiate it, apply `fit_transform` to the `current_numerical_features`, and update the DataFrame. Finally, I will display descriptive statistics to confirm the scaling.\n",
        "\n"
      ],
      "id": "0773ee56"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b30a76b"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Instantiate the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Apply fit_transform to the numerical features and update the DataFrame\n",
        "merged_data[current_numerical_features] = scaler.fit_transform(merged_data[current_numerical_features])\n",
        "\n",
        "print(\"Descriptive statistics for numerical columns after scaling:\")\n",
        "display(merged_data[current_numerical_features].describe())"
      ],
      "id": "0b30a76b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4eb1c54"
      },
      "source": [
        "## Define Target Variable\n",
        "\n",
        "### Subtask:\n",
        "Determine the target variable(s) for learning purchasing behavior. This could be future 'Quantity', 'Total_Cost', or a derived metric like customer churn, next purchase likelihood, or product recommendation.\n"
      ],
      "id": "f4eb1c54"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "635e3bd1"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Feature Merging and Preparation**:\n",
        "    *   Customer-centric aggregate features (e.g., `total_transactions`, `total_spending`) from `customer_aggregates` were successfully merged into the main dataset using `Company_ID`.\n",
        "    *   Product-centric aggregate features (e.g., `total_quantity_sold`, `total_revenue`) from `product_aggregates` were successfully merged using `Product_ID`.\n",
        "    *   After merging, the `merged_data` DataFrame expanded to 22 columns and contained 5329 rows.\n",
        "    *   Numerical features identified included `Transaction_ID`, `Company_ID`, `Product_ID`, `Quantity`, `Product_Price_x`, `Total_Cost`, `Company_Profit`, temporal features, and all aggregate features.\n",
        "    *   Categorical features identified were `Company_Name`, `Address`, and `Product_Name`.\n",
        "*   **Categorical Feature Encoding**:\n",
        "    *   The features `Company_Name`, `Address`, `Product_Name`, `Transaction_Month`, `Transaction_DayOfWeek`, and `Transaction_Quarter` were selected for encoding.\n",
        "    *   Temporal features (`Transaction_Month`, `Transaction_DayOfWeek`, `Transaction_Quarter`) were explicitly converted to `object` type to be treated as categorical.\n",
        "    *   One-hot encoding was applied to these features, resulting in a significant expansion of the DataFrame. The `merged_data` DataFrame's shape changed from 5329 rows and 22 columns to 5329 rows and 237 columns, indicating the creation of 215 new binary columns.\n",
        "    *   The original categorical columns were dropped after encoding.\n",
        "*   **Numerical Feature Scaling**:\n",
        "    *   All identified numerical features were scaled using `StandardScaler`.\n",
        "    *   Post-scaling descriptive statistics confirmed that the numerical features now have a mean very close to 0 and a standard deviation very close to 1, indicating successful standardization.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The dataset is now fully prepared with both categorical and numerical features transformed appropriately for machine learning model training.\n",
        "*   The next logical step is to proceed with model selection, training, and evaluation using this processed dataset.\n"
      ],
      "id": "635e3bd1"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "customers = pd.read_csv(\"customer_data.csv\")\n",
        "products = pd.read_csv(\"products_data.csv\")\n",
        "transactions = pd.read_csv(\"transactions_data.csv\")\n",
        "\n",
        "print(customers.shape)\n",
        "print(products.shape)\n",
        "print(transactions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h4Fqx-x6bxo",
        "outputId": "1b32ad62-e58b-40b9-c79d-ddbd02c7ee8c"
      },
      "id": "2h4Fqx-x6bxo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 15)\n",
            "(20, 4)\n",
            "(10000, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load customers and products with the correct separator\n",
        "customers = pd.read_csv(\"customer_data.csv\", sep=None, engine=\"python\")\n",
        "products  = pd.read_csv(\"products_data.csv\", sep=None, engine=\"python\")\n",
        "\n",
        "# Clean column names again\n",
        "customers.columns = customers.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "products.columns  = products.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
        "\n",
        "print(\"customers cols:\", list(customers.columns))\n",
        "print(\"products cols:\", list(products.columns))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdhdB2-H6dG4",
        "outputId": "f9b8fa65-449d-4008-9f65-a1349f06f910"
      },
      "id": "cdhdB2-H6dG4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customers cols: ['\\ufeff', 'company_id', 'total_orders', 'total_quantity', 'total_spent', 'avg_order_value', 'unique_products', 'last_purchase', 'first_purchase', 'qty_outlier_count', 'cost_outlier_count', 'recency_days', 'customer_age_days', 'avg_qty_per_order', 'will_buy_next_30d']\n",
            "products cols: ['\\ufeff', 'product_id', 'product_name', 'product_price']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove BOM artifact column if it exists\n",
        "if '\\ufeff' in customers.columns:\n",
        "    customers = customers.drop(columns=['\\ufeff'])\n",
        "\n",
        "if '\\ufeff' in products.columns:\n",
        "    products = products.drop(columns=['\\ufeff'])\n",
        "\n",
        "print(\"customers cols (fixed):\", list(customers.columns))\n",
        "print(\"products cols (fixed):\", list(products.columns))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr35ItDM6tyI",
        "outputId": "d6dcc834-947c-4db5-a73b-f1b79f5b111a"
      },
      "id": "sr35ItDM6tyI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "customers cols (fixed): ['company_id', 'total_orders', 'total_quantity', 'total_spent', 'avg_order_value', 'unique_products', 'last_purchase', 'first_purchase', 'qty_outlier_count', 'cost_outlier_count', 'recency_days', 'customer_age_days', 'avg_qty_per_order', 'will_buy_next_30d']\n",
            "products cols (fixed): ['product_id', 'product_name', 'product_price']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = pd.merge(transactions, customers, on=\"company_id\", how=\"left\")\n",
        "merged_data = pd.merge(merged_data, products, on=\"product_id\", how=\"left\")\n",
        "\n",
        "print(\"Merged shape:\", merged_data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjkCKojT6zAA",
        "outputId": "0be81faa-ce57-4213-c261-d5d97c58a36d"
      },
      "id": "MjkCKojT6zAA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged shape: (10000, 24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target variable\n",
        "TARGET_COL = \"high_value_transaction\"\n"
      ],
      "metadata": {
        "id": "uNnZgwiV7FMZ"
      },
      "id": "uNnZgwiV7FMZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop leakage and non-feature columns\n",
        "drop_cols = [\"transaction_id\", \"transaction_date\", \"total_cost\", \"unnamed:_0\"]\n",
        "drop_cols = [c for c in drop_cols if c in merged_data.columns]\n",
        "\n",
        "X = merged_data.drop(columns=[TARGET_COL] + drop_cols)\n",
        "y = merged_data[TARGET_COL]\n",
        "\n",
        "# Keep numeric features only\n",
        "X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "# Handle missing values from merge\n",
        "X = X.fillna(0)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCWrFdy77Hui",
        "outputId": "233f8c73-4341-41a0-f467-1efe3a26b02f"
      },
      "id": "LCWrFdy77Hui",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape: (10000, 16)\n",
            "y shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For baseline modeling we restrict to numeric features only; future work will encode categorical features."
      ],
      "metadata": {
        "id": "S3DtGzARbiLj"
      },
      "id": "S3DtGzARbiLj"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ],
      "metadata": {
        "id": "xejV_IE-7J_o",
        "outputId": "4f9f5c2c-69be-4ab9-fb9b-5ae35738a6e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "id": "xejV_IE-7J_o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1319031631.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\" Model trained: Logistic Regression\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWMh7jXd7L3_",
        "outputId": "b517b01c-22c4-4872-e980-d95b8804a0d8"
      },
      "id": "QWMh7jXd7L3_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model trained: Logistic Regression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8p1ZIty7OWX",
        "outputId": "1d1d7043-d0c2-4d53-ee21-9ab4c47a1ad9"
      },
      "id": "_8p1ZIty7OWX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.89\n",
            "\n",
            "Confusion Matrix:\n",
            " [[1399   95]\n",
            " [ 125  381]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.94      0.93      1494\n",
            "           1       0.80      0.75      0.78       506\n",
            "\n",
            "    accuracy                           0.89      2000\n",
            "   macro avg       0.86      0.84      0.85      2000\n",
            "weighted avg       0.89      0.89      0.89      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9XO5QPSnYm_X"
      },
      "id": "9XO5QPSnYm_X"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}